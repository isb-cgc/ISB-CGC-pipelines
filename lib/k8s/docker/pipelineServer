#!/usr/bin/env python
import os
import re
import json
import webapp2
import httplib2
import argparse
import requests

from googleapiclient import discovery
from oauth2client.client import GoogleCredentials
from googleapiclient.errors import HttpError

from StringIO import StringIO
from paste import httpserver
from pipelines.routes import *
from pipelines.queue import PipelineQueue, PipelineQueueError
from pipelines.schema import PipelineSchema, PipelineSchemaValidationError, PipelineSubmitError
from pipelines.builder import PipelineBuilder
from pipelines.config import PipelineConfig
from pipelines.service import PipelineService, PipelineServiceError
import pipelines.service.DataDisk
import pipelines.service.DataDiskError


CONFIG = PipelineConfig(path=os.environ["PIPELINES_CONFIG"])

SQLITE_READER_SERVICE_HOST = os.environ["SQLITE_READER_SERVICE_HOST"]
SQLITE_READER_SERVICE_PORT = os.environ["SQLITE_READER_SERVICE_PORT"]

SQLITE_WRITER_SERVICE_HOST = os.environ["SQLITE_WRITER_SERVICE_HOST"]
SQLITE_WRITER_SERVICE_PORT = os.environ["SQLITE_WRITER_SERVICE_PORT"]

CONFIG_WRITER_SERVICE_HOST = os.environ["CONFIG_WRITER_SERVICE_HOST"]


class Jobs(webapp2.RequestHandler):
	def get(self):
		if re.match(JOBS_GET_EXP, self.request.path):
			queryParams = {x.split('=')[0]: x.split('=')[1] for x in self.request.query_string.split('&')}

			try:
				req = {
					"select": queryParams.pop("select").split(','),
					"criteria": {
						"operation": queryParams.pop("operation"),
						"values": [{"key": k, "value": v} for k, v in queryParams.iteritems()]
					}
				}

			except KeyError:
				req = {
					"select": [],
					"criteria": {
						"operation": queryParams.pop("operation"),
						"values": [{"key": k, "value": v} for k, v in queryParams.iteritems()]
					}
				}

			try:
				jobsList = PipelineService.sendRequest(SQLITE_READER_SERVICE_HOST, SQLITE_READER_SERVICE_PORT, "/read/jobs", data=req, protocol="tcp")

			except PipelineServiceError as e:
				self.response.set_status(400, e)

			else:
				resp = {
					"jobs": jobsList.json()["data"]
				}

				status = 200

		else:
			status = 404
			resp = {
				"msg": "Resource {r} not found".format(r=self.request.path)
			}

		self.response.out = StringIO(json.dumps(resp))
		self.response.set_status(status)

	def post(self):
		kwargs = json.loads(self.request.body)
		resp = None
		status = None

		if re.match(JOBS_CREATE_EXP, self.request.path):
			pipelineSpec = PipelineSchema(CONFIG, **kwargs)
			pipelineBuilder = PipelineBuilder(CONFIG)
			pipelineBuilder.addStep(pipelineSpec)

			try:
				jobId = pipelineBuilder.run()

			except PipelineSchemaValidationError as e:
				status = 400

			except PipelineSubmitError as e:
				status = 400

			else:
				resp = {
					"job_id": jobId
				}
				self.response.set_status(201)

		elif re.match(JOB_EDIT_EXP, self.request.path):
			queryParams = {x.split('=')[0]: x.split('=')[1] for x in self.request.query_string.split('&')}
			req = {
				"updates": kwargs,
				"criteria": {
					"operation": "AND",
					"values": [
						{
							"key": "job_id",
							"value": queryParams["job_id"]
						}
					]
				}
			}

			try:
				PipelineService.sendRequest(SQLITE_WRITER_SERVICE_HOST, SQLITE_WRITER_SERVICE_PORT, "/update/jobs", data=req, protocol="tcp")

			except PipelineServiceError as e:
				status = 400
				resp = {
					"msg": "Couldn't edit job: {reason}".format(reason=e)
				}

			else:
				status = 204

		elif re.match(JOB_RESTART_EXP, self.request.path):
			pipelineQueue = PipelineQueue('WAIT_Q', host=os.environ["RABBITMQ_SERVICE_HOST"], port=os.environ["RABBITMQ_SERVICE_PORT"])

			req = {
				"select": ["job_id", "request"],
				"criteria": {
					"operation": operation,
					"values": [{"key": k, "value": v} for k, v in kwargs.iteritems()]
				}
			}

			try:
				jobInfo = PipelineService.sendRequest(SQLITE_READER_SERVICE_HOST, SQLITE_READER_SERVICE_PORT, "/jobs", data=req, protocol="tcp")

			except PipelineServiceError as e:
				status = 400
				resp = {
					"msg": "Couldn't restart job: {reason}".format(reason=e)
				}

			else:
				for j in jobInfo:
					msg = {
						"job_id": j["job_id"],
						"request": json.loads(j["request"])
					}

					try:
						pipelineQueue.publish(json.dumps(msg))

					except PipelineQueueError as e:
						raise PipelineSchedulerError("Couldn't restart pipeline: {reason}".format(reason=e))

		elif re.match(JOB_CANCEL_EXP, self.request.path):
			# stop/cancel a job
			PipelineScheduler.stopPipeline(CONFIG, **kwargs)

		else:
			status = 404
			resp = {
				"msg": "Resource {r} not found".format(r=self.request.path)
			}

		if resp is not None:
			self.response.out = StringIO(json.dumps(resp))

		self.response.set_status(status)

class Config(webapp2.RequestHandler):
	def post(self):
		data = json.load(self.request.body)
		resp = None
		status = None

		if re.match(CONFIG_EDIT_EXP, self.request.path):
			pass  # edit the config file

		else:
			self.response.set_status(404, "Resource {r} not found".format(r=self.request.path))

		if resp is not None:
			self.response.out = StringIO(json.dumps(resp))

		self.response.set_status(status)

	def get(self):
		data = json.load(self.request.body)
		resp = None
		status = None

		if re.match(CONFIG_VIEW_EXP, self.request.path):
			pass  # package up the config file contents and return

		else:
			self.response.set_status(404, "Resource {r} not found".format(r=self.request.path))

		if resp is not None:
			self.response.out = StringIO(json.dumps(resp))

		self.response.set_status(status)


class DataDisk(webapp2.RequestHandler):
	def post(self):
		data = json.load(self.request.body)
		resp = None
		status = None

		if re.match(DATADISKS_CREATE_EXP, self.request.path):
			# request format:
			# {
			#   "diskName": <disk-name>,
			#   "diskType": <disk-type>|None,
			#   "diskSize": <disk-size>,
			#   "zone": <zone>|None
			# }
			#

			try:
				pipelines.service.disks.DataDisk.create(CONFIG, **data)

			except pipelines.service.disks.DataDiskError as e:
				resp = {
					"msg": e
				}
				status = 400
			else:
				status = 204

		elif re.match(DATADISKE_DELETE_EXP, self.request.path):
			# request format:
			# {
			#   "disk_name": <disk-name>,
			#   "disk_type": <disk-type>|None,
			#   "disk_size": <disk-size>,
			#   "disk_zone": <zone>|None,
			#   "job_id": <job-id>
			# }
			#
			pipelines.service.DataDisk.delete()  # TODO: args

		else:
			self.response.set_status(404, "Resource {r} not found".format(r=self.request.path))

		if resp is not None:
			self.response.out = StringIO(json.dumps(resp))

		self.response.set_status(status)

	def get(self):
		data = json.loads(self.request.body)
		resp = None
		status = None

		if re.match(DATADISKS_LIST_EXP, self.request.path):
			results = PipelineService.sendRequest(SQLITE_READER_SERVICE_HOST, SQLITE_READER_SERVICE_PORT, "")  # TODO: args

		elif re.match(DATADISK_DESCRIBE_EXP, self.request.path):
			pass  # describe the data disk(s)

		else:
			self.response.set_status(404, "Resource {r} not found".format(r=self.request.path))

		if resp is not None:
			self.response.out = StringIO(json.dumps(resp))

		self.response.set_status(status)


class Log(webapp2.RequestHandler):
	def get(self):
		resp = None
		status = None

		if re.match(LOG_GET_EXP, self.request.path):
			jobId = os.path.basename(self.request.path)

			req = {
				"select": ["stdout_log", "stderr_log", "gcs_log_path"],
				"criteria": {
					"operation": "AND",
					"values": [
						{
							"key": "job_id",
							"value": jobId
						}
					]
				}
			}

			try:
				jobLogs = PipelineService.sendRequest(os.environ["SQLITE_READER_SERVICE_HOST"], os.environ["SQLITE_READER_SERVICE_PORT"], "/read/jobs", data=req, protocol="tcp")

			except PipelineServiceError as e:
				pass

			else:
				pass

		else:
			self.response.set_status(404, "Resource {r} not found".format(r=self.request.path))

		if resp is not None:
			self.response.out = StringIO(json.dumps(resp))

		self.response.set_status(status)


app = webapp2.WSGIApplication([
	(JOBS_LIST_EXP, Jobs),
	(JOBS_CREATE_EXP, Jobs),
	(JOB_DESCRIBE_EXP, Jobs),
	(JOB_EDIT_EXP, Jobs),
	(JOB_CANCEL_EXP, Jobs),
	(JOB_RESTART_EXP, Jobs),
	(CONFIG_VIEW_EXP, Config),
	(CONFIG_EDIT_EXP, Config),
	(DATADISKS_LIST_EXP, DataDisk),
	(DATADISKS_CREATE_EXP, DataDisk),
	(DATADISK_DESCRIBE_EXP, DataDisk),
	(DATADISK_DELETE_EXP, DataDisk),
	(LOG_STDOUT_EXP, Log),
	(LOG_STDERR_EXP, Log),
], debug=True)


def main(host, port):
	httpserver.serve(app, host=host, port=port)


if __name__ == "__main__":
	parser = argparse.ArgumentParser()
	parser.add_argument("--host", required=False, default="0.0.0.0")
	parser.add_argument("--port", required=False, default=80)
	args = parser.parse_args()
	main(args.host, args.port)
