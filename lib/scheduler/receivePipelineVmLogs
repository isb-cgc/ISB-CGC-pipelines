#!/usr/bin/env python
import os
import json
import base64
import httplib2
import argparse
import threading
from time import sleep
from apiclient import discovery
from dateutil.parser import parse
from googleapiclient.errors import HttpError
from oauth2client import client as oauth2client

from pipelines.config import PipelineConfig
from pipelines.db import DatabaseRecord
from pipelines.queue import PipelineQueue, PipelineChannel, PipelineQueueError
from pipelines.logger import PipelineJobLogger
from pipelines.routes import *
from pipelines.service import PipelineService, PipelineServiceError, DataDisk, DataDiskError

PUBSUB_SCOPES = ['https://www.googleapis.com/auth/pubsub']
CONFIG = PipelineConfig(path=os.environ["PIPELINES_CONFIG"])

RABBITMQ_SERVICE_HOST = os.environ["RABBITMQ_SERVICE_HOST"]
RABBITMQ_SERVICE_PORT = os.environ["RABBITMQ_SERVICE_PORT"]

SQLITE_READER_SERVICE_HOST = os.environ["SQLITE_READER_SERVICE_HOST"]
SQLITE_READER_SERVICE_PORT = os.environ["SQLITE_READER_SERVICE_PORT"]

SQLITE_WRITER_SERVICE_HOST = os.environ["SQLITE_WRITER_SERVICE_HOST"]
SQLITE_WRITER_SERVICE_PORT = os.environ["SQLITE_WRITER_SERVICE_PORT"]

CONFIG_WRITER_SERVICE_HOST = os.environ["CONFIG_WRITER_SERVICE_HOST"]


class PubsubMessageHandlers(object):
	@staticmethod
	def pipelineVmLogs(log, compute, genomics, conf):
		def getProcessingTime(status):
			# get the total processing time of the job
			processingStart = None
			processingTime = None
			for i, e in enumerate(status["metadata"]["events"]):
				if e["description"] == "running-docker":
					processingStart = status["metadata"]["events"][i]["startTime"]
					break

			if processingStart is not None:
				processingTimeDelta = parse(status["metadata"]["endTime"]) - parse(processingStart)
				processingTime = processingTimeDelta.total_seconds()

			return processingTime

		def getOperationStatus(operation):
			status = None
			while True:
				try:
					status = genomics.operations().get(name="operations/{op}".format(op=operation)).execute()
				except HttpError as e:
					if e.resp.status in [500, 502, 503, 504]:
						continue
					else:
						PipelineJobLogger.writeStderr("Couldn't get operation status : {reason}".format(reason=e))
				else:
					break

			return status

		pipelineQueue = PipelineQueue('WAIT_Q', host=RABBITMQ_SERVICE_HOST, port=RABBITMQ_SERVICE_PORT)
		pipelineChannel = PipelineChannel(host=RABBITMQ_SERVICE_HOST, port=RABBITMQ_SERVICE_PORT)

		instance = log["jsonPayload"]["resource"]["name"]

		if log["jsonPayload"]["event_subtype"] == "compute.instances.preempted":

			data = {
				"select": ["operation_id, job_id, pipeline_name, tag, create_time, current_status, request"],
				"criteria": {
					"operation": "AND",
					"values": [
						{
							"instance_name": instance
						}
					]
				}
			}

			try:
				resp = PipelineService.sendRequest(SQLITE_READER_SERVICE_HOST, SQLITE_READER_SERVICE_PORT, "/read/jobs", data=data)

			except PipelineServiceError as e:
				PipelineJobLogger.writeStderr("Instance {i} preempted, but no record exists in jobs database: {msg}".format(i=instance, msg=e))

			else:
				jobInfo = DatabaseRecord(resp["results"][0])
				operationId = jobInfo.operation_id
				PipelineJobLogger.writeStdout("Instance {i} (operation {o}) preempted!".format(i=instance, o=operationId))

				data = {
					"select": [
						"child_id"
					],
					"criteria": {
						"operation": "AND",
						"values": [
							{
								"parent_id": jobInfo.job_id
							}
						]
					}
				}

				try:
					resp = PipelineService.sendRequest(SQLITE_READER_SERVICE_HOST, SQLITE_READER_SERVICE_PORT, "/read/job_dependencies", data=data)
				except PipelineServiceError as e:
					PipelineJobLogger.writeStderr("Couldn't get job dependency info: {reason}".format(reason=e))

				else:
					results = json.loads(resp)["results"]
					children = [x["child_id"] for x in results]

					if jobInfo.current_status != "CANCELLED":
						data = {
							"updates": {
								"current_status": "PREEMPTED",
								"preemptions": {
									"incr": 1
								}
							},
							"criteria": {
								"job_id": jobInfo.job_id
							}
						}

						try:
							resp = PipelineService.sendRequest(SQLITE_WRITER_SERVICE_HOST, SQLITE_WRITER_SERVICE_PORT, "/update/jobs", data=data)

						except PipelineServiceError as e:
							PipelineJobLogger.writeStderr("Couldn't update job record: {reason}".format(reason=e))

						else:
							if CONFIG.autorestart_preempted:
								msg = {
									"job_id": jobInfo.job_id,
									"request": json.loads(jobInfo.request)
								}
								pipelineQueue.publish(json.dumps(msg))
							else:
								if len(children) > 0:
									PipelineJobLogger.writeStderr(
										"Couldn't start downstream jobs for job {j} ({pipeline}-{tag}) : status is PREEMPTED (autorestart is FALSE)".format(
											j=jobInfo.job_id, pipeline=jobInfo.pipeline_name, tag=jobInfo.tag))

								try:
									msg = {
										"current_status": "PREEMPTED"
									}
									pipelineChannel.publishToExchange('WATCH_EXCHANGE', "PIPELINE_JOB_{j}".format(j=jobInfo.job_id), msg)
								except PipelineQueueError as e:
									pass

					else:
						if len(children) > 0:
							PipelineJobLogger.writeStderr(
								"Couldn't start downstream jobs for job {j} ({pipeline}-{tag}) : status is CANCELLED".format(
									j=jobInfo.job_id, pipeline=jobInfo.pipeline_name, tag=jobInfo.tag))

						try:
							msg = {
								"current_status": "CANCELLED"
							}
							pipelineChannel.publishToExchange('WATCH_EXCHANGE', "PIPELINE_JOB_{j}".format(j=jobInfo.job_id), msg)
						except PipelineQueueError as e:
							pass

		elif log["jsonPayload"]["event_subtype"] == "compute.instances.delete":
			# update the status of the job in the jobs db
			data = {
				"select": ["job_id", "pipeline_name", "tag", "create_time", "operation_id", "current_status"],
				"criteria": {
					"operation": "AND",
					"values": [
						{
							"instance_name": instance
						}
					]
				}
			}

			try:
				resp = PipelineService.sendRequest(SQLITE_READER_SERVICE_HOST, SQLITE_READER_SERVICE_PORT, "/read/jobs", data=data)

			except PipelineServiceError as e:
				PipelineJobLogger.writeStderr("Instance {i} deleted, but no record exists in jobs database".format(i=instance))

			else:
				jobInfo = DatabaseRecord(resp["results"][0])
				operationId = jobInfo.operation_id
				currentStatus = jobInfo.current_status

				if not currentStatus == "PREEMPTED":
					status = getOperationStatus(operationId)
					if status is not None:
						while not "endTime" in status["metadata"].keys():
							sleep(5)
							status = getOperationStatus(operationId)

						if status is not None:
							processingTime = getProcessingTime(status)

							PipelineJobLogger.writeStdout("Instance {i} (operation {o}) completed!".format(i=instance, o=operationId))

							data = {
								"select": [
									"child_id"
								],
								"criteria": {
									"operation": "AND",
									"values": [
										{
											"parent_id": jobInfo.job_id
										}
									]
								}
							}

							try:
								resp = PipelineService.sendRequest(SQLITE_READER_SERVICE_HOST,
								                                   SQLITE_READER_SERVICE_PORT, "/read/job_dependencies",
								                                   data=data)
							except PipelineServiceError as e:
								PipelineJobLogger.writeStderr(
									"Couldn't get job dependency info: {reason}".format(reason=e))

							else:
								results = json.loads(resp)["results"]
								children = [x["child_id"] for x in results]

								if status["done"] and "error" not in status.keys():
									PipelineJobLogger.writeStdout("PIPELINE SUCCEEDED (pipeline: {pipeline}, tag: {tag}, operation: {operation})".format(pipeline=jobInfo.pipeline_name, tag=jobInfo.tag, operation=status["name"]))
									data = {
										"updates": {
											"current_status": "SUCCEEDED",
											"processing_time": processingTime,
											"end_time": status["metadata"]["endTime"]
										},
										"criteria": {
											"job_id": jobInfo.job_id
										}
									}

									try:
										resp = PipelineService.sendRequest(SQLITE_WRITER_SERVICE_HOST,
										                                   SQLITE_WRITER_SERVICE_PORT, "/update/jobs",
										                                   data=data)

									except PipelineServiceError as e:
										PipelineJobLogger.writeStderr(
											"Couldn't update job record: {reason}".format(reason=e))

									else:
										if len(children) > 0:
											for c in children:
												data = {
													"select": [
														"parent_id"
													],
													"criteria": {
														"operation": "AND",
														"values": [
															{
																"child_id": c
															}
														]
													}
												}

												try:
													resp = PipelineService.sendRequest(SQLITE_READER_SERVICE_HOST,
													                                   SQLITE_READER_SERVICE_PORT,
													                                   "/read/job_dependencies",
													                                   data=data)
												except PipelineServiceError as e:
													PipelineJobLogger.writeStderr(
														"Couldn't get job dependency info: {reason}".format(reason=e))

												else:
													results = json.loads(resp)["results"]
													parents = [x["parent_id"] for x in results]
													totalParents = len(parents)
													succeededParents = 1
													parents.remove(int(jobInfo.job_id))

													for p in parents:
														data = {
															"select": [
																"current_status"
															],
															"criteria": {
																"operation": "AND",
																"values": [
																	{
																		"job_id": p
																	}
																]
															}
														}

														try:
															resp = PipelineService.sendRequest(
																SQLITE_READER_SERVICE_HOST,
																SQLITE_READER_SERVICE_PORT,
																"/read/job_dependencies",
																data=data)
														except PipelineServiceError as e:
															PipelineJobLogger.writeStderr(
																"Couldn't get job status info: {reason}".format(
																	reason=e))

														else:
															status = json.loads(resp)["results"][0]["current_status"]

															if status == "SUCCEEDED":
																succeededParents += 1

															else:
																break

													if totalParents == succeededParents:
														data = {
															"select": [
																"request"
															],
															"criteria": {
																"operation": "AND",
																"values": [
																	{
																		"job_id": c
																	}
																]
															}
														}

														try:
															resp = PipelineService.sendRequest(
																SQLITE_READER_SERVICE_HOST,
																SQLITE_READER_SERVICE_PORT,
																"/read/jobs",
																data=data)
														except PipelineServiceError as e:
															PipelineJobLogger.writeStderr(
																"Couldn't get child job request: {reason}".format(
																	reason=e))

														else:
															childRequest = json.loads(resp)["results"][0]["request"]

															msg = {
																"job_id": c,
																"request": childRequest
															}
															pipelineQueue.publish(json.dumps(msg))

										else:
											PipelineJobLogger.writeStdout(
												"Job {jobid} has no child jobs to check!".format(jobid=jobInfo.job_id))

											data = {
												"select": ["disk_name", "disk_zone", "cleanup"],
												"criteria": {
													"operation": "AND",
													"values": {
														"job_id": jobInfo.job_id
													}
												}
											}

										try:
											resp = PipelineService.sendRequest(
												SQLITE_READER_SERVICE_HOST,
												SQLITE_READER_SERVICE_PORT,
												"/read/disks",
												data=data)

										except PipelineServiceError as e:
											PipelineJobLogger.writeStderr(
												"Couldn't get disk listing: {reason}".format(
													reason=e))

										else:
											for d in json.loads(resp)["results"]:
												if bool(d["cleanup"]):
													try:
														DataDisk.delete(conf, disk_name=d["disk_name"], disk_zone=d["disk_zone"])
													except DataDiskError as e:
														PipelineJobLogger.writeStderr(
															"Couldn't delete disk {name}: {reason}".format(
																disk=d["disk_name"], reason=e))

										try:
											msg = {
												"current_status": "SUCCEEDED"
											}
											pipelineChannel.publishToExchange('WATCH_EXCHANGE', "PIPELINE_JOB_{j}".format(j=jobInfo.job_id), msg)

										except PipelineQueueError as e:
											pass

								elif "error" in status.keys():
									if jobInfo.current_status != "CANCELLED":
										PipelineJobLogger.writeStdout("PIPELINE FAILED ({job}): {reason}".format(job="{pipeline}-{tag}".format(pipeline=jobInfo.pipeline_name, tag=jobInfo.tag), reason=status["error"]["message"]))

										data = {
											"updates": {
												"current_status": "FAILED",
												"processing_time": processingTime,
												"end_time": status["metadata"]["endTime"]
											},
											"criteria": {
												"job_id": jobInfo.job_id
											}
										}

										try:
											resp = PipelineService.sendRequest(SQLITE_WRITER_SERVICE_HOST,
											                                   SQLITE_WRITER_SERVICE_PORT,
											                                   "/update/jobs",
											                                   data=data)

										except PipelineServiceError as e:
											PipelineJobLogger.writeStderr(
												"Couldn't update job record: {reason}".format(reason=e))

										else:
											if len(children) > 0:
												PipelineJobLogger.writeStderr("Couldn't start downstream jobs for job {j} ({pipeline}-{tag}) : status is FAILED".format(
														j=jobInfo.job_id, pipeline=jobInfo.pipeline_name, tag=jobInfo.tag))

											try:
												msg = {
													"current_status": "FAILED"
												}
												pipelineChannel.publishToExchange('WATCH_EXCHANGE', "PIPELINE_JOB_{j}".format(j=jobInfo.job_id), msg)

											except PipelineQueueError as e:
												pass

					if status is None:
						data = {
							"updates": {
								"current_status": "UNKNOWN"
							},
							"criteria": {
								"job_id": jobInfo.job_id
							}
						}

						try:
							resp = PipelineService.sendRequest(SQLITE_WRITER_SERVICE_HOST,
							                                   SQLITE_WRITER_SERVICE_PORT, "/update/jobs",
							                                   data=data)

						except PipelineServiceError as e:
							PipelineJobLogger.writeStderr(
								"Couldn't update job record: {reason}".format(reason=e))

		elif log["jsonPayload"]["event_subtype"] == "compute.instances.insert":
			zone = log["resource"]["labels"]["zone"]
			try:
				instanceInfo = compute.instances().get(project=CONFIG.project_id, zone=zone, instance=instance).execute()

			except HttpError as e:
				PipelineJobLogger.writeStderr("Couldn't get info for instance {i} : {reason}".format(i=instance, reason=e))

			else:
				operationId = instanceInfo["description"].partition("Operation: ")[-1]
				data = {
					"select": ["job_id"],
					"criteria": {
						"operation": "AND",
						"values": [
							{
								"operation_id": operationId
							}
						]
					}
				}

				try:
					resp = PipelineService.sendRequest(SQLITE_READER_SERVICE_HOST, SQLITE_READER_SERVICE_PORT,
					                                   "/read/jobs", data=data)

				except PipelineServiceError as e:
					PipelineJobLogger.writeStderr(
						"Couldn't get job info: {reason}".format(reason=e))

				PipelineJobLogger.writeStdout("Instance {i} (operation {o}) started!".format(i=instance, o=operationId))

				data = {
					"updates": {
						"instance_name": instance
					},
					"criteria": {
						"operation_id": operationId
					}
				}

				try:
					resp = PipelineService.sendRequest(SQLITE_WRITER_SERVICE_HOST,
					                                   SQLITE_WRITER_SERVICE_PORT, "/update/jobs",
					                                   data=data)

				except PipelineServiceError as e:
					PipelineJobLogger.writeStderr(
						"Couldn't update job record: {reason}".format(reason=e))

				else:
					data = {
						"select": ["job_id"],
						"criteria": {
							"operation": "AND",
							"values": [
								{
									"instance_name": instance
								}
							]
						}
					}

					try:
						resp = PipelineService.sendRequest(SQLITE_READER_SERVICE_HOST, SQLITE_READER_SERVICE_PORT,
						                                   "/read/jobs", data=data)

					except PipelineServiceError:
						PipelineJobLogger.writeStderr(
							"Instance {i} deleted, but no record exists in jobs database".format(i=instance))

					else:
						try:
							msg = {
								"current_status": "RUNNING"
							}
							pipelineChannel.publishToExchange('WATCH_EXCHANGE', "PIPELINE_JOB_{j}".format(j=json.loads(resp)["results"][0]["job_id"]), msg)

						except PipelineQueueError as e:
							pass
							

def main(args, CONFIG):
	credentials = oauth2client.GoogleCredentials.get_application_default()

	if credentials.create_scoped_required():
		credentials = credentials.create_scoped(PUBSUB_SCOPES)

	http = httplib2.Http()
	credentials.authorize(http)

	pubsub = discovery.build('pubsub', 'v1', http=http)
	compute = discovery.build('compute', 'v1', http=http)
	genomics = discovery.build('genomics', 'v1alpha2', http=http)

	subscription = 'projects/{project}/subscriptions/{subscription}'.format(project=CONFIG.project_id, subscription=args.subscription)

	# Create a POST body for the Pub/Sub request
	body = {
		'returnImmediately': False,
		'maxMessages': 1
	}

	while True:
		if credentials.access_token_expired:
			credentials.refresh(http)

		try:
			resp = pubsub.projects().subscriptions().pull(subscription=subscription, body=body).execute()
		except HttpError as e:
				if e.resp.status == 504:
					continue
				else:
					PipelineJobLogger.writeStderr("Couldn't pull from subscription {s} : {reason}".format(s=args.subscription, reason=e))
		else:
			receivedMessages = resp.get('receivedMessages')
			if receivedMessages is not None:
				ackIds = []
				for receivedMessage in receivedMessages:
					pubsubMessage = receivedMessage.get('message')
					if pubsubMessage:
						log = json.loads(base64.b64decode(str(pubsubMessage.get('data'))))
						PipelineJobLogger.writeStdout("Received message: {msg}".format(msg=base64.b64decode(str(pubsubMessage.get('data')))))
						PubsubMessageHandlers.pipelineVmLogs(log, compute, genomics, CONFIG)
						ackIds.append(receivedMessage.get('ackId'))

				ackBody = {'ackIds': ackIds}

				# Acknowledge the message.
				pubsub.projects().subscriptions().acknowledge(subscription=subscription, body=ackBody).execute()


if __name__ == "__main__":
	parser = argparse.ArgumentParser()
	parser.add_argument("--subscription")

	args = parser.parse_args()
	CONFIG = PipelineConfig()

	t = threading.Thread(target=CONFIG.watch)
	t.daemon = True
	t.start()

	main(args, CONFIG)
	t.join()
