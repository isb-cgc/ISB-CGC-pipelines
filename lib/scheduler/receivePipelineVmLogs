#!/usr/bin/env python
import os
import json
import base64
import httplib2
import argparse
import threading
from time import sleep
from apiclient import discovery
from dateutil.parser import parse
from googleapiclient.errors import HttpError
from oauth2client import client as oauth2client

from pipelines.config import PipelineConfig
from pipelines.db import DatabaseRecord
from pipelines.queue import PipelineQueue, PipelineChannel, PipelineQueueError
from pipelines.logger import PipelineLogger
from pipelines.routes import *
from pipelines.service import PipelineService, PipelineServiceError

PUBSUB_SCOPES = ['https://www.googleapis.com/auth/pubsub']
CONFIG = PipelineConfig(path=os.environ["PIPELINES_CONFIG"])

RABBITMQ_SERVICE_HOST = os.environ["RABBITMQ_SERVICE_HOST"]
RABBITMQ_SERVICE_PORT = os.environ["RABBITMQ_SERVICE_PORT"]

SQLITE_READER_SERVICE_HOST = os.environ["SQLITE_READER_SERVICE_HOST"]
SQLITE_READER_SERVICE_PORT = os.environ["SQLITE_READER_SERVICE_PORT"]

SQLITE_WRITER_SERVICE_HOST = os.environ["SQLITE_WRITER_SERVICE_HOST"]
SQLITE_WRITER_SERVICE_PORT = os.environ["SQLITE_WRITER_SERVICE_PORT"]

CONFIG_WRITER_SERVICE_HOST = os.environ["CONFIG_WRITER_SERVICE_HOST"]


class PubsubMessageHandlers(object):
	@staticmethod
	def pipelineVmLogs(log, compute, genomics):
		def getProcessingTime(status):
			# get the total processing time of the job
			processingStart = None
			processingTime = None
			for i, e in enumerate(status["metadata"]["events"]):
				if e["description"] == "running-docker":
					processingStart = status["metadata"]["events"][i]["startTime"]
					break

			if processingStart is not None:
				processingTimeDelta = parse(status["metadata"]["endTime"]) - parse(processingStart)
				processingTime = processingTimeDelta.total_seconds()

			return processingTime

		#pipelineDatabase = PipelineDatabase(CONFIG)
		pipelineQueue = PipelineQueue('WAIT_Q', host=RABBITMQ_SERVICE_HOST, port=RABBITMQ_SERVICE_PORT)
		pipelineChannel = PipelineChannel(host=RABBITMQ_SERVICE_HOST, port=RABBITMQ_SERVICE_PORT)

		instance = log["jsonPayload"]["resource"]["name"]

		if log["jsonPayload"]["event_subtype"] == "compute.instances.preempted":
			query = '&'.join(["select=operation_id,job_id,pipeline_name,tag,create_time,current_status,request", "instance_name={i}".format(i=instance)])
			uri = "{route}?{query}"
			try:
				resp = PipelineService.sendRequest(SQLITE_READER_SERVICE_HOST, SQLITE_READER_SERVICE_PORT, uri.format(route=SQLITE_READ_JOBS, query=query))

			except PipelineServiceError as e:
				PipelineLogger.writeStderr("Instance {i} preempted, but no record exists in jobs database: {msg}".format(i=instance, msg=e))

			else:
				jobInfo = DatabaseRecord(resp["records"][0])
				operationId = jobInfo.operation_id
				PipelineLogger.writeStdout("Instance {i} (operation {o}) preempted!".format(i=instance, o=operationId))

				query = '&'.join([])  # TODO: fill in
				try:
					resp = PipelineService.sendRequest(SQLITE_READER_SERVICE_HOST, SQLITE_READER_SERVICE_PORT, uri.format(route=SQLITE_READ_JOB_DEPS, query=query))
					children = json.loads(resp)[""]  # TODO: specify
					#children = [x[0] for x in pipelineDatabase.getChildJobs(jobInfo.job_id)]

				if jobInfo.current_status != "CANCELLED":
					query = '&'.join([]) # TODO: fill in
					resp = PipelineService.sendRequest(SQLITE_WRITER_SERVICE_HOST, SQLITE_WRITER_SERVICE_PORT, uri.format(route=SQLITE_UPDATE_JOBS, query=query))
					#pipelineDatabase.updateJob(jobInfo.job_id, keyName="job_id", setValues={"current_status": "PREEMPTED", "preemptions": 1})

					if CONFIG.autorestart_preempted:
						msg = {
							"job_id": jobInfo.job_id,
							"request": json.loads(jobInfo.request)
						}
						pipelineQueue.publish(json.dumps(msg))
					else:
						if len(children) > 0:
							PipelineLogger.writeStderr(
								"Couldn't start downstream jobs for job {j} ({pipeline}-{tag}) : status is PREEMPTED (autorestart is FALSE)".format(
									j=jobInfo.job_id, pipeline=jobInfo.pipeline_name, tag=jobInfo.tag))

						try:
							msg = {
								"current_status": "PREEMPTED"
							}
							pipelineChannel.publishToExchange('WATCH_EXCHANGE', "PIPELINE_JOB_{j}".format(j=jobInfo.job_id), msg)
						except PipelineQueueError as e:
							pass

				else:
					if len(children) > 0:
						PipelineLogger.writeStderr(
							"Couldn't start downstream jobs for job {j} ({pipeline}-{tag}) : status is CANCELLED".format(
								j=jobInfo.job_id, pipeline=jobInfo.pipeline_name, tag=jobInfo.tag))

					try:
						msg = {
							"current_status": "CANCELLED"
						}
						pipelineChannel.publishToExchange('WATCH_EXCHANGE', "PIPELINE_JOB_{j}".format(j=jobInfo.job_id), msg)
					except PipelineQueueError as e:
						pass

		elif log["jsonPayload"]["event_subtype"] == "compute.instances.delete":
			# update the status of the job in the jobs db
			try:
				jobInfo = pipelineDatabase.getJobInfo(select=["job_id", "pipeline_name", "tag", "create_time", "operation_id", "current_status"], where={"instance_name": instance})[0]

			except IndexError:
				PipelineLogger.writeStderr("Instance {i} deleted, but no record exists in jobs database".format(i=instance))

			else:
				operationId = jobInfo.operation_id
				currentStatus = jobInfo.current_status

				if not currentStatus == "PREEMPTED":
					status = genomics.operations().get(name="operations/{op}".format(op=operationId)).execute()
					while not "endTime" in status["metadata"].keys():
						sleep(5)
						status = genomics.operations().get(name="operations/{op}".format(op=operationId)).execute()

					processingTime = getProcessingTime(status)

					PipelineLogger.writeStdout("Instance {i} (operation {o}) completed!".format(i=instance, o=operationId))

					children = [x[0] for x in pipelineDatabase.getChildJobs(jobInfo.job_id)]

					if status["done"] and "error" not in status.keys():
						PipelineLogger.writeStdout("PIPELINE SUCCEEDED (pipeline: {pipeline}, tag: {tag}, operation: {operation})".format(pipeline=jobInfo.pipeline_name, tag=jobInfo.tag, operation=status["name"]))
						pipelineDatabase.updateJob(jobInfo.job_id, keyName="job_id", setValues={"current_status": "SUCCEEDED", "end_time": status["metadata"]["endTime"], "processing_time": processingTime})

						if len(children) > 0:
							for c in children:
								parents = [x[0] for x in pipelineDatabase.getParentJobs(c)]
								totalParents = len(parents)
								succeededParents = 1
								parents.remove(int(jobInfo.job_id))

								for p in parents:
									status = pipelineDatabase.getJobInfo(select=["current_status"], where={"job_id": p})[0].current_status

									if status == "SUCCEEDED":
										succeededParents += 1

									else:
										break

								if totalParents == succeededParents:
									childRequest = json.loads(pipelineDatabase.getJobInfo(select=["request"], where={"job_id": c})[0].request)

									msg = {
										"job_id": c,
										"request": childRequest
									}
									pipelineQueue.publish(json.dumps(msg))

						else:
							PipelineLogger.writeStdout(
								"Job {jobid} has no child jobs to check!".format(jobid=jobInfo.job_id))

						try:
							msg = {
								"current_status": "SUCCEEDED"
							}
							pipelineChannel.publishToExchange('WATCH_EXCHANGE', "PIPELINE_JOB_{j}".format(j=jobInfo.job_id), msg)

						except PipelineQueueError as e:
							pass

					elif "error" in status.keys():
						if jobInfo.current_status != "CANCELLED":
							PipelineLogger.writeStdout("PIPELINE FAILED ({job}): {reason}".format(job="{pipeline}-{tag}".format(pipeline=jobInfo.pipeline_name, tag=jobInfo.tag), reason=status["error"]["message"]))
							pipelineDatabase.updateJob(jobInfo.job_id, keyName="job_id", setValues={"current_status": "FAILED", "end_time": status["metadata"]["endTime"], "processing_time": processingTime})
							if len(children) > 0:
								PipelineLogger.writeStderr("Couldn't start downstream jobs for job {j} ({pipeline}-{tag}) : status is FAILED".format(
										j=jobInfo.job_id, pipeline=jobInfo.pipeline_name, tag=jobInfo.tag))

							try:
								msg = {
									"current_status": "FAILED"
								}
								pipelineChannel.publishToExchange('WATCH_EXCHANGE', "PIPELINE_JOB_{j}".format(j=jobInfo.job_id), msg)

							except PipelineQueueError as e:
								pass

		elif log["jsonPayload"]["event_subtype"] == "compute.instances.insert":
			zone = log["resource"]["labels"]["zone"]
			try:
				instanceInfo = compute.instances().get(project=CONFIG.project_id, zone=zone, instance=instance).execute()

			except HttpError as e:
				PipelineLogger.writeStderr("Couldn't get info for instance {i} : {reason}".format(i=instance, reason=e))

			else:
				operationId = instanceInfo["description"].partition("Operation: ")[-1]
				try:
					jobId = pipelineDatabase.getJobInfo(select=["job_id"], where={"operation_id": operationId})[0].job_id
				except PipelineDatabaseError as e:
					pass # TODO: handle

				PipelineLogger.writeStdout("Instance {i} (operation {o}) started!".format(i=instance, o=operationId))
				pipelineDatabase.updateJob(operationId, keyName="operation_id", setValues={"instance_name": instance})

				try:
					jobInfo = pipelineDatabase.getJobInfo(select=["operation_id", "current_status"],
					                                      where={"instance_name": instance})[0]

				except IndexError:
					PipelineLogger.writeStderr(
						"Instance {i} deleted, but no record exists in jobs database".format(i=instance))

				else:
					try:
						msg = {
							"current_status": "RUNNING"
						}
						pipelineChannel.publishToExchange('WATCH_EXCHANGE', "PIPELINE_JOB_{j}".format(j=jobInfo.job_id), msg)

					except PipelineQueueError as e:
						pass
							

def main(args, CONFIG):
	credentials = oauth2client.GoogleCredentials.get_application_default()

	if credentials.create_scoped_required():
		credentials = credentials.create_scoped(PUBSUB_SCOPES)

	http = httplib2.Http()
	credentials.authorize(http)

	pubsub = discovery.build('pubsub', 'v1', http=http)
	compute = discovery.build('compute', 'v1', http=http)
	genomics = discovery.build('genomics', 'v1alpha2', http=http)

	subscription = 'projects/{project}/subscriptions/{subscription}'.format(project=CONFIG.project_id, subscription=args.subscription)

	# Create a POST body for the Pub/Sub request
	body = {
		'returnImmediately': False,
		'maxMessages': 1
	}

	while True:
		if credentials.access_token_expired:
			credentials.refresh(http)

		try:
			resp = pubsub.projects().subscriptions().pull(subscription=subscription, body=body).execute()
		except HttpError as e:
				if e.resp.status == 504:
					continue
				else:
					PipelineLogger.writeStderr("Couldn't pull from subscription {s} : {reason}".format(s=args.subscription, reason=e))
		else:
			receivedMessages = resp.get('receivedMessages')
			if receivedMessages is not None:
				ackIds = []
				for receivedMessage in receivedMessages:
					pubsubMessage = receivedMessage.get('message')
					if pubsubMessage:
						log = json.loads(base64.b64decode(str(pubsubMessage.get('data'))))
						PipelineLogger.writeStdout("Received message: {msg}".format(msg=base64.b64decode(str(pubsubMessage.get('data')))))
						PubsubMessageHandlers.pipelineVmLogs(log, compute, genomics, CONFIG)
						ackIds.append(receivedMessage.get('ackId'))

				ackBody = {'ackIds': ackIds}

				# Acknowledge the message.
				pubsub.projects().subscriptions().acknowledge(subscription=subscription, body=ackBody).execute()


if __name__ == "__main__":
	parser = argparse.ArgumentParser()
	parser.add_argument("--subscription")

	args = parser.parse_args()
	CONFIG = PipelineConfig()

	t = threading.Thread(target=CONFIG.watch)
	t.daemon = True
	t.start()

	main(args, CONFIG)
	t.join()
