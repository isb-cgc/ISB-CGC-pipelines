#!/usr/bin/env python
import json
import base64
import httplib2
import argparse
import threading
from time import sleep
from apiclient import discovery
from dateutil.parser import parse
from googleapiclient.errors import HttpError
from oauth2client import client as oauth2client

from pipelines.utils import PipelinesConfig, PipelineDbUtils, PipelineQueueUtils, PipelineSchedulerUtils

PUBSUB_SCOPES = ['https://www.googleapis.com/auth/pubsub']


class PubsubMessageHandlers(object):
	@staticmethod
	def pipelineVmLogs(log, compute, genomics, config):
		def getProcessingTime(status):
			# get the total processing time of the job
			processingStart = None
			processingTime = None
			for i, e in enumerate(status["metadata"]["events"]):
				if e["description"] == "running-docker":
					processingStart = status["metadata"]["events"][i]["startTime"]
					break

			if processingStart is not None:
				processingTimeDelta = parse(status["metadata"]["endTime"]) - parse(processingStart)
				processingTime = processingTimeDelta.total_seconds()

			return processingTime

		def getOperationStatus(operation):
			status = None
			while True:
				try:
					status = genomics.operations().get(name="operations/{op}".format(op=operation)).execute()
				except HttpError as e:
					if e.resp.status in [500, 502, 503, 504]:
						continue
					else:
						PipelineSchedulerUtils.writeStderr("Couldn't get operation status : {reason}".format(reason=e))
				else:
					break

			return status

		pipelineDbUtils = PipelineDbUtils(config)
		pipelineQueueUtils = PipelineQueueUtils('WAIT_Q')

		instance = log["jsonPayload"]["resource"]["name"]

		if log["jsonPayload"]["event_subtype"] == "compute.instances.preempted":

			try:
				jobInfo = pipelineDbUtils.getJobInfo(select=["operation_id", "job_id", "pipeline_name", "tag", "create_time", "current_status", "request"], where={"instance_name": instance})[0]

			except IndexError:
				PipelineSchedulerUtils.writeStderr("Instance {i} preempted, but no record exists in jobs database".format(i=instance))

			else:
				operationId = jobInfo.operation_id
				PipelineSchedulerUtils.writeStdout("Instance {i} (operation {o}) preempted!".format(i=instance, o=operationId))

				children = [x[0] for x in pipelineDbUtils.getChildJobs(jobInfo.job_id)]

				if jobInfo.current_status != "CANCELLED":
					pipelineDbUtils.updateJob(jobInfo.job_id, keyName="job_id", setValues={"current_status": "PREEMPTED", "preemptions": 1})

					if config.autorestart_preempted:
						msg = {
							"job_id": jobInfo.job_id,
							"request": json.loads(jobInfo.request)
						}
						pipelineQueueUtils.publish(json.dumps(msg))
					else:
						if len(children) > 0:
							PipelineSchedulerUtils.writeStderr(
								"Couldn't start downstream jobs for job {j} ({pipeline}-{tag}) : status is PREEMPTED (autorestart is FALSE)".format(
									j=jobInfo.job_id, pipeline=jobInfo.pipeline_name, tag=jobInfo.tag))

				else:
					if len(children) > 0:
						PipelineSchedulerUtils.writeStderr(
							"Couldn't start downstream jobs for job {j} ({pipeline}-{tag}) : status is CANCELLED".format(
								j=jobInfo.job_id, pipeline=jobInfo.pipeline_name, tag=jobInfo.tag))

		elif log["jsonPayload"]["event_subtype"] == "compute.instances.delete":
			# update the status of the job in the jobs db
			try:
				jobInfo = pipelineDbUtils.getJobInfo(select=["job_id", "pipeline_name", "tag", "create_time", "operation_id", "current_status"], where={"instance_name": instance})[0]

			except IndexError:
				PipelineSchedulerUtils.writeStderr("Instance {i} deleted, but no record exists in jobs database".format(i=instance))

			else:
				operationId = jobInfo.operation_id
				currentStatus = jobInfo.current_status

				if not currentStatus == "PREEMPTED":
					status = getOperationStatus(operationId)
					if status is not None:
						while not "endTime" in status["metadata"].keys():
							sleep(5)
							status = getOperationStatus(operationId)

						if status is not None:
							processingTime = getProcessingTime(status)

							PipelineSchedulerUtils.writeStdout("Instance {i} (operation {o}) completed!".format(i=instance, o=operationId))

							children = [x[0] for x in pipelineDbUtils.getChildJobs(jobInfo.job_id)]

							if status["done"] and "error" not in status.keys():
								PipelineSchedulerUtils.writeStdout("PIPELINE SUCCEEDED (pipeline: {pipeline}, tag: {tag}, operation: {operation})".format(pipeline=jobInfo.pipeline_name, tag=jobInfo.tag, operation=status["name"]))
								pipelineDbUtils.updateJob(jobInfo.job_id, keyName="job_id", setValues={"current_status": "SUCCEEDED", "end_time": status["metadata"]["endTime"], "processing_time": processingTime})

								if len(children) > 0:
									for c in children:
										parents = [x[0] for x in pipelineDbUtils.getParentJobs(c)]
										totalParents = len(parents)
										succeededParents = 1
										parents.remove(int(jobInfo.job_id))

										for p in parents:
											status = pipelineDbUtils.getJobInfo(select=["current_status"], where={"job_id": p})[0].current_status

											if status == "SUCCEEDED":
												succeededParents += 1

											else:
												break

										if totalParents == succeededParents:
											childRequest = json.loads(pipelineDbUtils.getJobInfo(select=["request"], where={"job_id": c})[0].request)

											msg = {
												"job_id": c,
												"request": childRequest
											}
											pipelineQueueUtils.publish(json.dumps(msg))

								else:
									PipelineSchedulerUtils.writeStdout(
										"Job {jobid} has no child jobs to check!".format(jobid=jobInfo.job_id))

							elif "error" in status.keys():
								if jobInfo.current_status != "CANCELLED":
									PipelineSchedulerUtils.writeStdout("PIPELINE FAILED ({job}): {reason}".format(job="{pipeline}-{tag}".format(pipeline=jobInfo.pipeline_name, tag=jobInfo.tag), reason=status["error"]["message"]))
									pipelineDbUtils.updateJob(jobInfo.job_id, keyName="job_id", setValues={"current_status": "FAILED", "end_time": status["metadata"]["endTime"], "processing_time": processingTime})
									if len(children) > 0:
										PipelineSchedulerUtils.writeStderr("Couldn't start downstream jobs for job {j} ({pipeline}-{tag}) : status is FAILED".format(
												j=jobInfo.job_id, pipeline=jobInfo.pipeline_name, tag=jobInfo.tag))

					if status is None:
						pipelineDbUtils.updateJob(jobInfo.job_id, setValues={"current_status": "UNKNOWN"}, keyName="job_id")

		elif log["jsonPayload"]["event_subtype"] == "compute.instances.insert":
			zone = log["resource"]["labels"]["zone"]
			try:
				operationId = compute.instances().get(project=config.project_id, zone=zone, instance=instance).execute()["description"].partition("Operation: ")[-1]

			except HttpError as e:
				PipelineSchedulerUtils.writeStderr("Couldn't get the operation id for instance {i} : {reason}".format(i=instance, reason=e))

			else:
				PipelineSchedulerUtils.writeStdout("Instance {i} (operation {o}) started!".format(i=instance, o=operationId))
				pipelineDbUtils.updateJob(operationId, keyName="operation_id", setValues={"instance_name": instance})
							

def main(args, config):
	credentials = oauth2client.GoogleCredentials.get_application_default()

	if credentials.create_scoped_required():
		credentials = credentials.create_scoped(PUBSUB_SCOPES)

	http = httplib2.Http()
	credentials.authorize(http)

	pubsub = discovery.build('pubsub', 'v1', http=http)
	compute = discovery.build('compute', 'v1', http=http)
	genomics = discovery.build('genomics', 'v1alpha2', http=http)

	subscription = 'projects/{project}/subscriptions/{subscription}'.format(project=config.project_id, subscription=args.subscription)

	# Create a POST body for the Pub/Sub request
	body = {
		'returnImmediately': False,
		'maxMessages': 1
	}

	while True:
		if credentials.access_token_expired:
			credentials.refresh(http)

		try:
			resp = pubsub.projects().subscriptions().pull(subscription=subscription, body=body).execute()
		except HttpError as e:
				if e.resp.status in [500, 502, 503, 504]:
					continue
				else:
					PipelineSchedulerUtils.writeStderr("Couldn't pull from subscription {s} : {reason}".format(s=args.subscription, reason=e))
		else:
			receivedMessages = resp.get('receivedMessages')
			if receivedMessages is not None:
				ackIds = []
				for receivedMessage in receivedMessages:
					pubsubMessage = receivedMessage.get('message')
					if pubsubMessage:
						log = json.loads(base64.b64decode(str(pubsubMessage.get('data'))))
						PipelineSchedulerUtils.writeStdout("Received message: {msg}".format(msg=base64.b64decode(str(pubsubMessage.get('data')))))
						PubsubMessageHandlers.pipelineVmLogs(log, compute, genomics, config)
						ackIds.append(receivedMessage.get('ackId'))

				ackBody = {'ackIds': ackIds}

				# Acknowledge the message.
				pubsub.projects().subscriptions().acknowledge(subscription=subscription, body=ackBody).execute()


if __name__ == "__main__":
	parser = argparse.ArgumentParser()
	parser.add_argument("--config")
	parser.add_argument("--subscription")

	args = parser.parse_args()
	config = PipelinesConfig(path=args.config)

	t = threading.Thread(target=config.watch)
	t.daemon = True
	t.start()

	main(args, config)
	t.join()
