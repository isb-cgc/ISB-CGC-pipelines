#!/usr/bin/env python
import os
import json
import base64
import httplib2
import argparse
import threading
from time import sleep
from apiclient import discovery
from dateutil.parser import parse
from googleapiclient.errors import HttpError
from oauth2client import client as oauth2client

from pipelines.config import PipelineConfig
from pipelines.db import PipelineDatabase
from pipelines.queue import PipelineQueue, PipelineChannel, PipelineQueueError
from pipelines.logger import PipelineLogger

PUBSUB_SCOPES = ['https://www.googleapis.com/auth/pubsub']


class PubsubMessageHandlers(object):
	@staticmethod
	def pipelineVmLogs(log, compute, genomics, config):
		def getProcessingTime(status):
			# get the total processing time of the job
			processingStart = None
			processingTime = None
			for i, e in enumerate(status["metadata"]["events"]):
				if e["description"] == "running-docker":
					processingStart = status["metadata"]["events"][i]["startTime"]
					break

			if processingStart is not None:
				processingTimeDelta = parse(status["metadata"]["endTime"]) - parse(processingStart)
				processingTime = processingTimeDelta.total_seconds()

			return processingTime

		def getOperationStatus(operation):
			status = None
			while True:
				try:
					status = genomics.operations().get(name="operations/{op}".format(op=operation)).execute()
				except HttpError as e:
					if e.resp.status in [500, 502, 503, 504]:
						continue
					else:
						PipelineLogger.writeStderr("Couldn't get operation status : {reason}".format(reason=e))
				else:
					break

			return status

		pipelineDatabase = PipelineDatabase(config)
		pipelineQueue = PipelineQueue('WAIT_Q', host=os.environ["RABBITMQ_SERVICE_HOST"], port=os.environ["RABBITMQ_SERVICE_PORT"])
		pipelineChannel = PipelineChannel(host=os.environ["RABBITMQ_SERVICE_HOST"], port=os.environ["RABBITMQ_SERVICE_PORT"])

		instance = log["jsonPayload"]["resource"]["name"]

		if log["jsonPayload"]["event_subtype"] == "compute.instances.preempted":

			try:
				jobInfo = pipelineDatabase.getJobInfo(select=["operation_id", "job_id", "pipeline_name", "tag", "create_time", "current_status"], where={"instance_name": instance})[0]

			except IndexError:
				PipelineLogger.writeStderr("Instance {i} preempted, but no record exists in jobs database".format(i=instance))

			else:
				operationId = jobInfo.operation_id
				PipelineLogger.writeStdout("Instance {i} (operation {o}) preempted!".format(i=instance, o=operationId))

				children = [x[0] for x in pipelineDatabase.getChildJobs(jobInfo.job_id)]

				if jobInfo.current_status != "CANCELLED":
					pipelineDatabase.updateJob(jobInfo.job_id, keyName="job_id", setValues={"current_status": "PREEMPTED", "preemptions": 1})

					if config.autorestart_preempted:
						jobInfo = pipelineDatabase.getJobInfo(select=["request", "job_id"], where={"operation_id": operationId})
						msg = {
							"job_id": jobInfo.job_id,
							"request": json.loads(jobInfo.request)
						}
						pipelineQueue.publish(json.dumps(msg))
					else:
						if len(children) > 0:
							PipelineLogger.writeStderr(
								"Couldn't start downstream jobs for job {j} ({pipeline}-{tag}) : status is PREEMPTED (autorestart is FALSE)".format(
									j=jobInfo.job_id, pipeline=jobInfo.pipeline_name, tag=jobInfo.tag))

						try:
							msg = {
								"current_status": "PREEMPTED"
							}
							pipelineChannel.publishToExchange('WATCH_EXCHANGE', "PIPELINE_JOB_{j}".format(j=jobInfo.job_id), msg)
						except PipelineQueueError as e:
							pass

				else:
					if len(children) > 0:
						PipelineLogger.writeStderr(
							"Couldn't start downstream jobs for job {j} ({pipeline}-{tag}) : status is CANCELLED".format(
								j=jobInfo.job_id, pipeline=jobInfo.pipeline_name, tag=jobInfo.tag))

					try:
						msg = {
							"current_status": "CANCELLED"
						}
						pipelineChannel.publishToExchange('WATCH_EXCHANGE', "PIPELINE_JOB_{j}".format(j=jobInfo.job_id), msg)
					except PipelineQueueError as e:
						pass

		elif log["jsonPayload"]["event_subtype"] == "compute.instances.delete":
			# update the status of the job in the jobs db
			try:
				jobInfo = pipelineDatabase.getJobInfo(select=["job_id", "pipeline_name", "tag", "create_time", "operation_id", "current_status"], where={"instance_name": instance})[0]

			except IndexError:
				PipelineLogger.writeStderr("Instance {i} deleted, but no record exists in jobs database".format(i=instance))

			else:
				operationId = jobInfo.operation_id
				currentStatus = jobInfo.current_status

				if not currentStatus == "PREEMPTED":
					status = getOperationStatus(operationId)
					if status is not None:
						while not "endTime" in status["metadata"].keys():
							sleep(5)
							status = getOperationStatus(operationId)

						if status is not None:
							processingTime = getProcessingTime(status)

							PipelineLogger.writeStdout("Instance {i} (operation {o}) completed!".format(i=instance, o=operationId))

							children = [x[0] for x in pipelineDatabase.getChildJobs(jobInfo.job_id)]

							if status["done"] and "error" not in status.keys():
								PipelineLogger.writeStdout("PIPELINE SUCCEEDED (pipeline: {pipeline}, tag: {tag}, operation: {operation})".format(pipeline=jobInfo.pipeline_name, tag=jobInfo.tag, operation=status["name"]))
								pipelineDatabase.updateJob(jobInfo.job_id, keyName="job_id", setValues={"current_status": "SUCCEEDED", "end_time": status["metadata"]["endTime"], "processing_time": processingTime})

								if len(children) > 0:
									for c in children:
										parents = [x[0] for x in pipelineDatabase.getParentJobs(c)]
										totalParents = len(parents)
										succeededParents = 1
										parents.remove(int(jobInfo.job_id))

										for p in parents:
											status = pipelineDatabase.getJobInfo(select=["current_status"], where={"job_id": p})[0].current_status

											if status == "SUCCEEDED":
												succeededParents += 1

											else:
												break

										if totalParents == succeededParents:
											childRequest = json.loads(pipelineDatabase.getJobInfo(select=["request"], where={"job_id": c})[0].request)

											msg = {
												"job_id": c,
												"request": childRequest
											}
											pipelineQueue.publish(json.dumps(msg))

								else:
									PipelineLogger.writeStdout(
										"Job {jobid} has no child jobs to check!".format(jobid=jobInfo.job_id))

								try:
									msg = {
										"current_status": "SUCCEEDED"
									}
									pipelineChannel.publishToExchange('WATCH_EXCHANGE', "PIPELINE_JOB_{j}".format(j=jobInfo.job_id), msg)

								except PipelineQueueError as e:
									pass

							elif "error" in status.keys():
								if jobInfo.current_status != "CANCELLED":
									PipelineLogger.writeStdout("PIPELINE FAILED ({job}): {reason}".format(job="{pipeline}-{tag}".format(pipeline=jobInfo.pipeline_name, tag=jobInfo.tag), reason=status["error"]["message"]))
									pipelineDatabase.updateJob(jobInfo.job_id, keyName="job_id", setValues={"current_status": "FAILED", "end_time": status["metadata"]["endTime"], "processing_time": processingTime})
									if len(children) > 0:
										PipelineLogger.writeStderr("Couldn't start downstream jobs for job {j} ({pipeline}-{tag}) : status is FAILED".format(
												j=jobInfo.job_id, pipeline=jobInfo.pipeline_name, tag=jobInfo.tag))

									try:
										msg = {
											"current_status": "FAILED"
										}
										pipelineChannel.publishToExchange('WATCH_EXCHANGE', "PIPELINE_JOB_{j}".format(j=jobInfo.job_id), msg)

									except PipelineQueueError as e:
										pass

					if status is None:
						pipelineDatabase.updateJob(jobInfo.job_id, setValues={"current_status": "UNKNOWN"}, keyName="job_id")

		elif log["jsonPayload"]["event_subtype"] == "compute.instances.insert":
			zone = log["resource"]["labels"]["zone"]
			try:
				instanceInfo = compute.instances().get(project=config.project_id, zone=zone, instance=instance).execute()

			except HttpError as e:
				PipelineLogger.writeStderr("Couldn't get info for instance {i} : {reason}".format(i=instance, reason=e))

			else:
				operationId = instanceInfo["description"].partition("Operation: ")[-1]
				PipelineLogger.writeStdout("Instance {i} (operation {o}) started!".format(i=instance, o=operationId))
				pipelineDatabase.updateJob(operationId, keyName="operation_id", setValues={"instance_name": instance})

				try:
					jobInfo = pipelineDatabase.getJobInfo(select=["operation_id", "current_status"],
					                                      where={"instance_name": instance})[0]

				except IndexError:
					PipelineLogger.writeStderr(
						"Instance {i} deleted, but no record exists in jobs database".format(i=instance))

				else:
					try:
						msg = {
							"current_status": "RUNNING"
						}
						pipelineChannel.publishToExchange('WATCH_EXCHANGE', "PIPELINE_JOB_{j}".format(j=jobInfo.job_id), msg)

					except PipelineQueueError as e:
						pass
							

def main(args, config):
	credentials = oauth2client.GoogleCredentials.get_application_default()

	if credentials.create_scoped_required():
		credentials = credentials.create_scoped(PUBSUB_SCOPES)

	http = httplib2.Http()
	credentials.authorize(http)

	pubsub = discovery.build('pubsub', 'v1', http=http)
	compute = discovery.build('compute', 'v1', http=http)
	genomics = discovery.build('genomics', 'v1alpha2', http=http)

	subscription = 'projects/{project}/subscriptions/{subscription}'.format(project=config.project_id, subscription=args.subscription)

	# Create a POST body for the Pub/Sub request
	body = {
		'returnImmediately': False,
		'maxMessages': 1
	}

	while True:
		if credentials.access_token_expired:
			credentials.refresh(http)

		try:
			resp = pubsub.projects().subscriptions().pull(subscription=subscription, body=body).execute()
		except HttpError as e:
				if e.resp.status == 504:
					continue
				else:
					PipelineLogger.writeStderr("Couldn't pull from subscription {s} : {reason}".format(s=args.subscription, reason=e))
		else:
			receivedMessages = resp.get('receivedMessages')
			if receivedMessages is not None:
				ackIds = []
				for receivedMessage in receivedMessages:
					pubsubMessage = receivedMessage.get('message')
					if pubsubMessage:
						log = json.loads(base64.b64decode(str(pubsubMessage.get('data'))))
						PipelineLogger.writeStdout("Received message: {msg}".format(msg=base64.b64decode(str(pubsubMessage.get('data')))))
						PubsubMessageHandlers.pipelineVmLogs(log, compute, genomics, config)
						ackIds.append(receivedMessage.get('ackId'))

				ackBody = {'ackIds': ackIds}

				# Acknowledge the message.
				pubsub.projects().subscriptions().acknowledge(subscription=subscription, body=ackBody).execute()


if __name__ == "__main__":
	parser = argparse.ArgumentParser()
	parser.add_argument("--subscription")

	args = parser.parse_args()
	config = PipelineConfig()

	t = threading.Thread(target=config.watch)
	t.daemon = True
	t.start()

	main(args, config)
	t.join()
