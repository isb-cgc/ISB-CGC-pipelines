#!/usr/bin/env python

# !/usr/bin/env python
import json
import httplib2
import threading

from apiclient.discovery import build
from googleapiclient.errors import HttpError
from oauth2client.client import GoogleCredentials
from pipelines.config import PipelineConfig
from pipelines.db import PipelineDatabase
from pipelines.queue import PipelineQueue
from pipelines.logger import PipelineJobLogger


# NOTE: this process should be started up as part of the scheduling system (managed by Supervisor)


def main(config):
	# authenticate
	credentials = GoogleCredentials.get_application_default()
	http = httplib2.Http()
	credentials.authorize(http)

	genomics = build('genomics', 'v1alpha2', http=http)

	pipelineDatabase = PipelineDatabase(config)
	pipelineQueue = PipelineQueue('CANCEL_Q')

	while True:
		# consume a request
		body, method = pipelineQueue.get()

		if method:
			body = json.loads(body)

			jobInfo = pipelineDatabase.getJobInfo(select=["current_status", "operation_id"], where={"job_id": body["job_id"]})[0]

			if jobInfo.current_status != "CANCELLED":
				try:
					genomics.operations().cancel(name="operations/{o}".format(o=jobInfo.operation_id), body={}).execute()
				except HttpError as e:
					jobInfo = pipelineDatabase.getJobInfo(select=["pipeline_name", "tag"], where={"job_id": body["job_id"]})
					PipelineJobLogger.writeStderr(
						"ERROR: couldn't cancel job {pipeline}-{tag} : {reason}".format(
							pipeline=jobInfo[0].pipeline_name, tag=jobInfo[0].tag, reason=e))
				else:
					pipelineDatabase.updateJob(body["job_id"], keyName="job_id", setValues={"current_status": "CANCELLED"})

			pipelineQueue.acknowledge(method)

		else:
			pass


if __name__ == "__main__":
	config = PipelineConfig()

	t = threading.Thread(target=config.watch)
	t.daemon = True
	t.start()

	main(config)
	t.join()









