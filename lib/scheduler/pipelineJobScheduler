#!/usr/bin/env python
import os
import json
import httplib2
import argparse
import threading
from time import sleep
from apiclient.discovery import build
from googleapiclient.errors import HttpError
from oauth2client.client import GoogleCredentials
from pipelines.utils import PipelinesConfig, PipelineSchedulerUtils, PipelineDbUtils, PipelineQueueUtils

# NOTE: this process should be started up as part of the scheduling system (managed by Supervisor)


def main(config):
	# authenticate
	credentials = GoogleCredentials.get_application_default()
	http = httplib2.Http()
	credentials.authorize(http)

	compute = build('compute', 'v1', http=http)
	genomics = build('genomics', 'v1alpha2', http=http)

	pipelineDbUtils = PipelineDbUtils(config)
	pipelineQueueUtils = PipelineQueueUtils('WAIT_Q')

	while True:
		# consume a request
		body, method = pipelineQueueUtils.get()

		if method:
			body = json.loads(body)

			# get quota information
			cpuUsage = 0
			hddUsage = 0
			ssdUsage = 0

			cpuTotal = 0
			hddTotal = 0
			ssdTotal = 0

			if credentials.access_token_expired:
				credentials.refresh(http)

			maxJobsTest = -1
			cpuTest = -1
			hddTest = -1
			ssdTest = -1

			# if the quota isn't maxed, and the number of currently running jobs is less than the user defined max, start the job
			while not cpuTest and hddTest and ssdTest and maxJobsTest:
				for z in config.zones.split(','):
					region = os.path.basename(compute.zones().get(project=config.project_id, zone=z).execute()["region"])
					quotas = compute.regions().get(project=config.project_id, region=region)["quotas"]
					for q in quotas:
						if q["metric"] == "CPUS":
							cpuTotal += q["limit"]
							cpuUsage = q["usage"]
						if q["metric"] == "DISKS_TOTAL_GB":
							hddTotal += q["limit"]
							hddUsage += q["usage"]
						if q["metric"] == "SSD_TOTAL_GB":
							ssdTotal += q["limit"]
							ssdUsage += q["usage"]

				currentlyRunning = len(pipelineDbUtils.getJobInfo(select=["job_id"], where={"current_status": "RUNNING"}))
				maxJobsTest = config.max_running_jobs - (currentlyRunning + 1)
				cpuTest = cpuTotal - (cpuUsage + body["pipelineArgs"]["resources"]["minimumCpuCores"])
				hddTest = hddTotal - (hddUsage + 10)  # each job uses at least 10GB of HDD by default
				ssdTest = 1

				for d in body["pipelineArgs"]["resources"]["disks"]:
					if d["type"] == "PERSISTENT_SSD":
						ssdTest = ssdTotal - (ssdUsage + d["sizeGb"])
					elif d["type"] == "PERSISTENT_HDD":
						hddTest -= d["sizeGb"]

			jobInfo = pipelineDbUtils.getJobInfo(select=["current_status"], where={"job_id": body["job_id"]})[0]

			if jobInfo.current_status != "CANCELLED":
				try:
					run = genomics.pipelines().run(body=body["request"]).execute()
				except HttpError as e:
					jobInfo = pipelineDbUtils.getJobInfo(select=["pipeline_name", "tag"], where={"job_id": body["job_id"]})
					PipelineSchedulerUtils.writeStderr(
						"ERROR: couldn't start job {pipeline}-{tag} : {reason}".format(pipeline=jobInfo[0].pipeline_name, tag=jobInfo[0].tag, reason=e))
					pipelineDbUtils.updateJob(body["job_id"], keyName="job_id", setValues={"current_status": "ERROR"})

				else:
					operationId = run["name"].split("/")[-1]
					PipelineSchedulerUtils.writeStdout("Operation submitted: {op}".format(op=operationId))
					pipelineDbUtils.updateJob(body["job_id"], keyName="job_id", setValues={"current_status": "RUNNING", "operation_id": operationId, "stdout_log": "{op}-stdout.log".format(op=operationId), "stderr_log": "{op}-stderr.log".format(op=operationId), "create_time": run["metadata"]["createTime"], "end_time": None, "processing_time": None})

			pipelineQueueUtils.acknowledge(method)

		else:
			pass

if __name__ == "__main__":
	parser = argparse.ArgumentParser()
	parser.add_argument("--config")

	args = parser.parse_args()
	config = PipelinesConfig(args.config)

	t = threading.Thread(target=config.watch)
	t.daemon = True
	t.start()

	main(config)
	t.join()





		



